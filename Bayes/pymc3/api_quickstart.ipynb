{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markbaum/Python/django/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model creation\n",
    "\n",
    "Models in PyMC3 are centered around the `Model` class. It has references to all random variables (RVs) and computes the model logp and its gradients. Usually, you would instantiate it as part of a `with` context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Model definition\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss RVs further below but let's create a simple model to explore the `Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=1, observed=np.random.randn(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mu, obs]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mu]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.free_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[obs]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.observed_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-134.3314561)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logp({'mu': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**\n",
    "It's worth highlighting one of the counter-intuitive design choices with logp. \n",
    "The API makes the `logp` look like an attribute, when it actually puts together a function based on the current state of the model. \n",
    "\n",
    "The current design is super maintainable, does terrible if the state stays constant, and great if the state keeps changing, for reasons of design we assume that `Model` isn't static, in fact it's best in our experience and avoids bad results. \n",
    "\n",
    "If you need to use `logp` in an inner loop and it needs to be static, simply use something like `logp = model.logp` below. You can see the caching effect with the speed up below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.8 ms ± 2.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "22.7 µs ± 515 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.logp({mu: 0.1})\n",
    "logp = model.logp\n",
    "%timeit logp({mu: 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probability Distributions\n",
    "\n",
    "Every probabilistic program consists of observed and unobserved Random Variables (RVs). Observed RVs are defined via likelihood distributions, while unobserved RVs are defined via prior distributions. In PyMC3, probability distributions are available from the main module space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Normal in module pymc3.distributions.continuous:\n",
      "\n",
      "class Normal(pymc3.distributions.distribution.Continuous)\n",
      " |  Univariate normal log-likelihood.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     f(x \\mid \\mu, \\tau) =\n",
      " |         \\sqrt{\\frac{\\tau}{2\\pi}}\n",
      " |         \\exp\\left\\{ -\\frac{\\tau}{2} (x-\\mu)^2 \\right\\}\n",
      " |  \n",
      " |  ========  ==========================================\n",
      " |  Support   :math:`x \\in \\mathbb{R}`\n",
      " |  Mean      :math:`\\mu`\n",
      " |  Variance  :math:`\\dfrac{1}{\\tau}` or :math:`\\sigma^2`\n",
      " |  ========  ==========================================\n",
      " |  \n",
      " |  Normal distribution can be parameterized either in terms of precision\n",
      " |  or standard deviation. The link between the two parametrizations is\n",
      " |  given by\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     \\tau = \\dfrac{1}{\\sigma^2}\n",
      " |  \n",
      " |  .. plot::\n",
      " |  \n",
      " |      import matplotlib.pyplot as plt\n",
      " |      import numpy as np\n",
      " |      import scipy.stats as st\n",
      " |      x = np.linspace(-5.0, 5.0, 1000)\n",
      " |      fig, ax = plt.subplots()\n",
      " |      f = lambda mu, sd : st.norm.pdf(x, loc=mu, scale=sd)\n",
      " |      plot_pdf = lambda a, b : ax.plot(x, f(a,b), label=r'$\\mu$={0}, $\\sigma$={1}'.format(a,b))\n",
      " |      plot_pdf(0.0, 0.4)\n",
      " |      plot_pdf(0.0, 1.0)\n",
      " |      plot_pdf(0.0, 2.0)\n",
      " |      plot_pdf(-2.0, 0.4)\n",
      " |      plt.legend(loc='upper right', frameon=False)\n",
      " |      ax.set(xlim=[-5,5], ylim=[0,1.2], xlabel='x', ylabel='f(x)')\n",
      " |      plt.show()\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  mu : float\n",
      " |      Mean.\n",
      " |  sd : float\n",
      " |      Standard deviation (sd > 0).\n",
      " |  tau : float\n",
      " |      Precision (tau > 0).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normal\n",
      " |      pymc3.distributions.distribution.Continuous\n",
      " |      pymc3.distributions.distribution.Distribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mu=0, sd=None, tau=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  logp(self, value)\n",
      " |  \n",
      " |  random(self, point=None, size=None, repeat=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __latex__ = _repr_latex_(self, name=None, dist=None)\n",
      " |      Magic method name for IPython to use for LaTeX formatting.\n",
      " |  \n",
      " |  default(self)\n",
      " |  \n",
      " |  get_test_val(self, val, defaults)\n",
      " |  \n",
      " |  getattr_value(self, val)\n",
      " |  \n",
      " |  logp_nojac(self, *args, **kwargs)\n",
      " |      Return the logp, but do not include a jacobian term for transforms.\n",
      " |      \n",
      " |      If we use different parametrizations for the same distribution, we\n",
      " |      need to add the determinant of the jacobian of the transformation\n",
      " |      to make sure the densities still describe the same distribution.\n",
      " |      However, MAP estimates are not invariant with respect to the\n",
      " |      parametrization, we need to exclude the jacobian terms in this case.\n",
      " |      \n",
      " |      This function should be overwritten in base classes for transformed\n",
      " |      distributions.\n",
      " |  \n",
      " |  logp_sum(self, *args, **kwargs)\n",
      " |      Return the sum of the logp values for the given observations.\n",
      " |      \n",
      " |      Subclasses can use this to improve the speed of logp evaluations\n",
      " |      if only the sum of the logp values is needed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  dist(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __new__(cls, name, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.Normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PyMC3 module, the structure for probability distributions looks like this:\n",
    "\n",
    "[pymc3.distributions](../api/distributions.rst)\n",
    "- [continuous](../api/distributions/continuous.rst)\n",
    "- [discrete](../api/distributions/discrete.rst)\n",
    "- [timeseries](../api/distributions/timeseries.rst)\n",
    "- [mixture](../api/distributions/mixture.rst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Discrete',\n",
       " 'Distribution',\n",
       " 'Mixture',\n",
       " 'Normal',\n",
       " 'NormalMixture',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'all_discrete',\n",
       " 'bound',\n",
       " 'draw_values',\n",
       " 'generate_samples',\n",
       " 'get_tau_sd',\n",
       " 'get_variable_name',\n",
       " 'logsumexp',\n",
       " 'np',\n",
       " 'tt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pm.distributions.mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unobserved Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every unobserved RV has the following calling signature: name (str), parameter keyword arguments. Thus, a normal prior can be defined in a model context like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sd=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the model, we can evaluate its logp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0.91893853)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.logp({'x': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observed RVs are defined just like unobserved RVs but require data to be passed into the `observed` keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    obs = pm.Normal('x', mu=0, sd=1, observed=np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`observed` supports lists, `numpy.ndarray`, `theano` and `pandas` data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3 allows you to freely do algebra with RVs in all kinds of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sd=1)\n",
    "    y = pm.Gamma('y', alpha=1, beta=1)\n",
    "    plus_2 = x + 2\n",
    "    summed = x + y\n",
    "    squared = x**2\n",
    "    sined = pm.math.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these transformations work seamlessly, its results are not stored automatically. Thus, if you want to keep track of a transformed variable, you have to use `pm.Determinstic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sd=1)\n",
    "    plus_2 = pm.Deterministic('x plus 2', x + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `plus_2` can be used in the identical way to above, we only tell PyMC3 to keep track of this RV for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic transforms of bounded RVs\n",
    "\n",
    "In order to sample models more efficiently, PyMC3 automatically transforms bounded RVs to be unbounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform('x', lower=0, upper=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the RVs of the model, we would expect to find `x` there, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x_interval__]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.free_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_interval__` represents `x` transformed to accept parameter values between -inf and +inf. In the case of an upper and a lower bound, a `LogOdd`s transform is applied. Sampling in this transformed space makes it easier for the sampler. PyMC3 also keeps track of the non-transformed, bounded parameters. These are common determinstics (see above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deterministics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying results, PyMC3 will usually hide transformed parameters. You can pass the `include_transformed=True` parameter to many functions to see the transformed parameters that are used for sampling.\n",
    "\n",
    "You can also turn transforms off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x]\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform('x', lower=0, upper=1, transform=None)\n",
    "    \n",
    "print(model.free_RVs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists of RVs / higher-dimensional RVs\n",
    "\n",
    "Above we have seen how to create scalar RVs. In many models, you want multiple RVs. There is a tendency (mainly inherited from PyMC 2.x) to create list of RVs, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = [pm.Normal('x_{}'.format(i), mu=0, sd=1) for i in range(10)] # bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even though this works it is quite slow and not recommended. Instead, use the `shape` kwarg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal('x', mu=0, sd=1, shape=10) # good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is now a random vector of length 10. We can index into it or do linear algebra operations on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    y = x[0] * x[1] # full indexing is supported\n",
    "    x.dot(x.T) # Linear algebra is supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization with test_values\n",
    "\n",
    "While PyMC3 tries to automatically initialize models it is sometimes helpful to define initial values for RVs. This can be done via the `testval` kwarg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sd=1, shape=5)\n",
    "\n",
    "x.tag.test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1770293 ,  0.32102193, -2.02090431, -0.11231715,  1.23042394])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sd=1, shape=5, testval=np.random.randn(5))\n",
    "\n",
    "x.tag.test_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is quite useful to identify problems with model specification or initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference\n",
    "\n",
    "Once we have defined our model, we have to perform inference to approximate the posterior distribution. PyMC3 supports two broad classes of inference: sampling and variational inference.\n",
    "\n",
    "### 3.1 Sampling\n",
    "\n",
    "The main entry point to MCMC sampling algorithms is via the `pm.sample()` function. By default, this function tries to auto-assign the right sampler(s) and auto-initialize if you don't pass anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/home/markbaum/Python/django/lib/python3.6/site-packages/pymc3/model.py:384: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if not np.issubdtype(var.dtype, float):\n",
      "100%|██████████| 1500/1500 [00:01<00:00, 1349.72it/s]\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=1, observed=np.random.randn(100))\n",
    "    \n",
    "    trace = pm.sample(1000, tune=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on a continuous model, PyMC3 assigns the NUTS sampler, which is very efficient even for complex models. PyMC3 also runs variational inference (i.e. ADVI) to find good starting parameters for the sampler. Here we draw 1000 samples from the posterior and allow the sampler to adjust its parameters in an additional 500 iterations. These 500 samples are discarded by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run multiple chains in parallel using the `cores` kwarg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/home/markbaum/Python/django/lib/python3.6/site-packages/pymc3/model.py:384: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if not np.issubdtype(var.dtype, float):\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2399.09it/s]\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=1, observed=np.random.randn(100))\n",
    "    \n",
    "    trace = pm.sample(cores=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we are now drawing 2000 samples, 500 samples for 4 chains each. The 500 tuning samples are discarded by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace['mu'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.nchains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Python/django/lib/python3.6/site-packages/pymc3/backends/base.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self, varname, burn, thin, combine, chains, squeeze)\u001b[0m\n\u001b[1;32m    411\u001b[0m             results = [self._straces[chain].get_values(varname, burn, thin)\n\u001b[0;32m--> 412\u001b[0;31m                        for chain in chains]\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single chain passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-da2b668a96d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m# get values of a single chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Python/django/lib/python3.6/site-packages/pymc3/backends/base.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self, varname, burn, thin, combine, chains, squeeze)\u001b[0m\n\u001b[1;32m    412\u001b[0m                        for chain in chains]\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single chain passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_straces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvarname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "trace.get_values('mu', chains=1).shape # get values of a single chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC3, offers a variety of other samplers, found in `pm.step_methods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: x[0].isupper(), dir(pm.step_methods)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used step-methods besides NUTS are `Metropolis` and `Slice`. **For almost all continuous models, `NUTS` should be preferred.** There are hard-to-sample models for which `NUTS` will be very slow causing many users to use `Metropolis` instead. This practice, however, is rarely successful. NUTS is fast on simple models but can be slow if the model is very complex or it is badly initialized. In the case of a complex model that is hard for NUTS, Metropolis, while faster, will have a very low effective sample size or not converge properly at all. A better approach is to instead try to improve initialization of NUTS, or reparameterize the model.\n",
    "\n",
    "For completeness, other sampling methods can be passed to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=1, observed=np.random.randn(100))\n",
    "    \n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also assign variables to different step methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    sd = pm.HalfNormal('sd', sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=sd, observed=np.random.randn(100))\n",
    "    \n",
    "    step1 = pm.Metropolis(vars=[mu])\n",
    "    step2 = pm.Slice(vars=[sd])\n",
    "    trace = pm.sample(10000, step=[step1, step2], cores=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyze sampling results\n",
    "\n",
    "The most common used plot to analyze sampling results is the so-called trace-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common metric to look at is R-hat, also known as the Gelman-Rubin statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.gelman_rubin(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are also part of the `forestplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.forestplot(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for a plot of the posterior that is inspired by the book [Doing Bayesian Data Analysis](http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/), you can use the:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For high-dimensional models it becomes cumbersome to look at all parameter's traces. When using `NUTS` we can look at the energy plot to assess problems of convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal('x', mu=0, sd=1, shape=100)    \n",
    "    trace = pm.sample(cores=4)\n",
    "    \n",
    "pm.energyplot(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on sampler stats and the energy plot, see [here](sampler-stats.ipynb). For more information on identifying sampling problems and what to do about them, see [here](Diagnosing_biased_Inference_with_Divergences.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Variational inference\n",
    "\n",
    "PyMC3 supports various Variational Inference techniques. While these methods are much faster, they are often also less accurate and can lead to biased inference. The main entry point is `pymc3.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    sd = pm.HalfNormal('sd', sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=sd, observed=np.random.randn(100))\n",
    "    \n",
    "    approx = pm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `Approximation` object has various capabilities, like drawing samples from the approximated posterior, which we can analyse like a regular sampling run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx.sample(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `variational` submodule offers a lot of flexibility in which VI to use and follows an object oriented design. For example, full-rank ADVI estimates a full covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pm.floatX([0., 0.])\n",
    "cov = pm.floatX([[1, .5], [.5, 1.]])\n",
    "with pm.Model() as model:\n",
    "    pm.MvNormal('x', mu=mu, cov=cov, shape=2)\n",
    "    approx = pm.fit(method='fullrank_advi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent expression using the object-oriented interface is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.MvNormal('x', mu=mu, cov=cov, shape=2)\n",
    "    approx = pm.FullRankADVI().fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trace = approx.sample(10000)\n",
    "sns.kdeplot(trace['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stein Variational Gradient Descent (SVGD) uses particles to estimate the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pm.floatX([.2, .8])\n",
    "mu = pm.floatX([-.3, .5])\n",
    "sd = pm.floatX([.1, .1])\n",
    "with pm.Model() as model:\n",
    "    pm.NormalMixture('x', w=w, mu=mu, sd=sd)\n",
    "    approx = pm.fit(method=pm.SVGD(n_particles=200, jitter=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trace = approx.sample(10000)\n",
    "sns.distplot(trace['x']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on variational inference, see [these examples](http://pymc-devs.github.io/pymc3/examples.html#variational-inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior Predictive Sampling\n",
    "\n",
    "The `sample_ppc()` function performs prediction on hold-out data and posterior predictive checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(100)\n",
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sd=1)\n",
    "    sd = pm.HalfNormal('sd', sd=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sd=sd, observed=data)\n",
    "    \n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    post_pred = pm.sample_ppc(trace, samples=500, size=len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample_ppc()` returns a dict with a key for every observed node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred['obs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = sns.distplot(post_pred['obs'].mean(axis=1), label='Posterior predictive means')\n",
    "ax.axvline(data.mean(), color='r', ls='--', label='True mean')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Predicting on hold-out data\n",
    "\n",
    "In many cases you want to predict on unseen / hold-out data. This is especially relevant in Probabilistic Machine Learning and Bayesian Deep Learning. While we plan to improve the API in this regard, this can currently be achieved with a `theano.shared` variable. These are theano tensors whose values can be changed later. Otherwise they can be passed into PyMC3 just like any other numpy array or tensor.\n",
    "\n",
    "This distinction is significant since internally all models in PyMC3 are giant symbolic expressions. When you pass data directly into a model, you are giving Theano permission to treat this data as a constant and optimize it away as it sees fit. If you need to change this data later you might not have a way to point at it in the symbolic expression. Using `theano.shared` offers a way to point to a place in that symbolic expression, and change what is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "x = np.random.randn(100)\n",
    "y = x > 0\n",
    "\n",
    "x_shared = theano.shared(x)\n",
    "y_shared = theano.shared(y)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    coeff = pm.Normal('x', mu=0, sd=1)\n",
    "    logistic = pm.math.sigmoid(coeff * x_shared)\n",
    "    pm.Bernoulli('obs', p=logistic, observed=y_shared)\n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume we want to predict on unseen data. For this we have to change the values of `x_shared` and `y_shared`. Theoretically we don't need to set `y_shared` as we want to predict it but it has to match the shape of `x_shared`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shared.set_value([-1, 0, 1.])\n",
    "y_shared.set_value([0, 0, 0]) # dummy values\n",
    "\n",
    "with model:\n",
    "    post_pred = pm.sample_ppc(trace, samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred['obs'].mean(axis=0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
